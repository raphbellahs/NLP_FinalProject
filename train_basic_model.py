from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer
from datasets import *
import numpy as np
import sacrebleu
from project_evaluate import *
import os


def load_dataset(files):
    """
        This function loads a list of text files containing English and German texts and creates a list of datasets.
        Each dataset contains a list of translations, where each translation is a dictionary with 'en' and 'de' keys
        for the English and German text respectively.

        Parameters:
        files (list): A list of file paths to be loaded.

        Returns:
        list: A list of datasets, where each dataset is a Dataset object containing a list of translations.
        """
    raw_datasets = []
    for path in files:
        with open(path, 'r', encoding='utf8') as f:
            dataset = {'translation': []}
            en_data, de_data =read_file(path)
            for en_sen, de_sen in zip(en_data, de_data):
                dataset['translation'].append({'de': de_sen, 'en': en_sen})
            # Create a Dataset object from the dictionary and add it to the list
            dataset = Dataset.from_dict(dataset)
            raw_datasets.append(dataset)
    return raw_datasets

def preprocess_function(dataset):
    """
        This function preprocesses a dataset of translations by tokenizing the inputs and targets using a tokenizer object.
        The inputs are generated by concatenating a prefix with the source language sentences,
        while the targets are the target language sentences.

        Parameters:
        dataset (Dataset): A Dataset object containing a list of translations.

        Returns:
        dict: A dictionary containing tokenized model inputs and labels for the translation task.
        The dictionary contains the following keys:
        - input_ids: A list of tokenized input sequences.
        - attention_mask: A list of binary values indicating which tokens should be attended to by the model.
        - labels: A list of tokenized target sequences.
        """
    inputs = [prefix + sentence[source_lang] for sentence in dataset["translation"]]
    targets = [sentence[target_lang] for sentence in dataset["translation"]]
    model_inputs = tokenizer(inputs, max_length=500, truncation=True)
    return model_inputs

def compute_metrics(eval_preds):
    """
    This function computes evaluation metrics for a translation task.
    The function uses the sacrebleu library to compute the BLEU score between the predicted and target translations.
    The function also computes the average length of the predicted translations.

    Parameters:
    eval_preds (tuple): A tuple containing the predicted and target translations.
    The predicted translations are in the first element of the tuple,
    while the target translations are in the second element.

    Returns:
    dict: A dictionary containing the evaluation metrics for the translation task.
    The dictionary contains the following keys:
    - bleu: The BLEU score between the predicted and target translations.
    - gen_len: The average length of the predicted translations.
    """
    metric = evaluate.load("sacrebleu")
    preds, labels = eval_preds
    if isinstance(preds, tuple):
        preds = preds[0]
    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

    # Replace -100 in the labels as we can't decode them.
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Some simple post-processing
    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
    result = {"bleu": result["score"]}

    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
    result["gen_len"] = np.mean(prediction_lens)
    result = {k: round(v, 4) for k, v in result.items()}
    return result

if __name__ == '__main__':
    print('entered in the main')
    TRAIN = 'data/train.labeled'
    VAL = 'data/val.labeled'

    DF = load_dataset([TRAIN, VAL])
    raw_datasets = DatasetDict({"train": DF[0], "validation": DF[1]})

    model_name = "t5-base"
    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    max_input_length = 500
    max_target_length = 500
    source_lang = "de"
    target_lang = "en"
    prefix = "translate German to English: "

    tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
    tokenized_datasets.set_format('torch')
    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

    # Train:
    print('Training Started!')
    os.environ['TOKENIZERS_PARALLELISM'] = 'True'
    batch_size = 4
    args = Seq2SeqTrainingArguments(
        output_dir="t5-base-7epoch",
        evaluation_strategy="epoch",
        learning_rate=2e-4,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        weight_decay=0.01,
        save_total_limit=3,
        num_train_epochs=1,
        predict_with_generate=True,
        fp16=True,
        logging_steps=1000,
        generation_max_length=500,
    )
    trainer = Seq2SeqTrainer(
        model,
        args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    trainer.train()
    quit()
    trainer.save_model("t5-base-7epoch-model")
    model.save_pretrained("/content/model-t5-base-7epoch")
    tokenizer.save_pretrained("/content/tokenizer-t7-base-5epoch")
    tokenized_datasets.save_to_disk('tokenized_datasets')